{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d415f0-cef7-4fe5-b8c4-8331e5f6da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import BinaryIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dbe0ee3-4b7d-45b3-8680-8fec32d1d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./tokenizer/bpe_example.txt', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4696854a-9050-4f74-83f2-3ccb49fe2296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.tell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8f1d405-efd7-456f-8e51-9ecd04e2c95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'low low lo'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = f.read(10)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7484a503-7480-4823-bc91-44385a321b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e464d-8409-46e2-aa7b-1bf05d6180a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(\n",
    "        input_path: str | os.PathLike,\n",
    "        vocab_size: int,\n",
    "        special_tokens: list[str]\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    path: Path = Path(input_path)\n",
    "    context: str\n",
    "    with path.open('+r'):\n",
    "        \n",
    "    return {}, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ef5109d-b83a-461f-b0da-44c8f8563e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = './tokenizer/bpe_example.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e17b1c90-f358-4928-9561-7681dd57a188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'low low low low low\\nlower lower widest widest widest\\nnewest newest newest newest newest newest'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "path: Path = Path(p)\n",
    "content: str = ''\n",
    "with path.open('+r') as f:\n",
    "    content = f.read()\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bf5987a-4aa5-461c-8137-a54396e7c1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'low low low low low\\nlower lower widest widest widest\\nnewest newest newest newest newest newest'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c68fe751-6dd5-4fd5-9a55-d0cb54044fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "def pretokenizer(content: str, special_tokens: list[str]) -> dict[list[bytes], int]:\n",
    "    matches = re.finditer(PAT, content)\n",
    "    counter = {}\n",
    "    for match in matches:\n",
    "        text = match.group()\n",
    "        counter[text] = counter.get(text, 0) + 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f2c63ad-9ade-43f1-8503-a1a5589546ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'low': 1,\n",
       " ' low': 4,\n",
       " '\\n': 2,\n",
       " 'lower': 1,\n",
       " ' lower': 1,\n",
       " ' widest': 3,\n",
       " 'newest': 1,\n",
       " ' newest': 5}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretokenizer(content, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6cb7b3dd-dc79-4c9b-be41-1530ea56f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_tokens(content: str, special_tokens: list[str]) -> list[str]:\n",
    "    pattern = '|'.join(re.escape(d) for d in special_tokens)\n",
    "    return re.split(pattern, content)\n",
    "\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "\n",
    "def pretokenizer(content: str, special_tokens: list[str]) -> dict[list[bytes], int]:\n",
    "    docs = remove_special_tokens(content, special_tokens)\n",
    "    counter = {}\n",
    "    for doc in docs:\n",
    "        matches = re.finditer(PAT, doc)\n",
    "\n",
    "        for match in matches:\n",
    "            text = tuple(match.group().encode('utf8'))\n",
    "            counter[text] = counter.get(text, 0) + 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951fe4e-da91-4cb3-9b57-e2dd78fbc4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('data/TinyStoriesV2-GPT4-valid.txt')\n",
    "with path.open('+r') as f:\n",
    "    content = f.read()\n",
    "    print(pretokenizer(content, ['<|endoftext|>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26c6346a-99e9-495c-96a7-9f8d9bba409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41e2ae5a-f6e8-4d73-901f-247ad3bc5a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x00'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a416224-4424-4cb1-90b9-17c27b34d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(256):\n",
    "    print(bytes([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52f019c0-5639-4bbb-85d5-d4193529d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vocab(special_tokens: list[str]) -> dict[int, bytes]:\n",
    "    vocab = {}\n",
    "    for i in range(256):\n",
    "        vocab[i] = bytes([i])\n",
    "    for st in special_tokens:\n",
    "        idx = len(vocab)\n",
    "        vocab[idx] = st.encode('utf8')\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a26789ef-760d-4691-88b5-849f8e8bdd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'<|endoftext|>'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_vocab(['<|endoftext|>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6837cb07-dc26-4999-b9af-fb12a2d5aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_bp(pretokenized_count: dict[tuple[bytes], int]):\n",
    "    # Count byte pairs\n",
    "    bp_counter = {}\n",
    "    for bytes_tuple, count in pretokenized_count.items():\n",
    "        for i in range(len(bytes_tuple) - 1):\n",
    "            bp = (bytes_tuple[i], bytes_tuple[i+1])\n",
    "            bp_counter[bp] = bp_counter.get(bp, 0) + count\n",
    "    max_bp, max_count = (bytes([0]), bytes([0])), 0\n",
    "    for bp, bp_count in bp_counter.items():\n",
    "        if bp_count > max_count:\n",
    "            max_bp = bp\n",
    "            max_count = bp_count\n",
    "        if bp_count == max_count and bp > max_bp:\n",
    "            max_bp = bp\n",
    "            max_count = bp_count\n",
    "    return max_bp, max_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "84c00c4b-c2f2-4fe5-83b2-e3c68ad156a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretokenized_count():\n",
    "    path = Path('data/TinyStoriesV2-GPT4-valid.txt')\n",
    "    with path.open('+r') as f:\n",
    "        content = f.read()\n",
    "        return pretokenizer(content, ['<|endoftext|>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5456d741-9dbf-495b-bd9a-d3413da5443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretokenized_count = get_pretokenized_count()\n",
    "max_bp, max_count = find_max_bp(pretokenized_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "05fae080-3ace-4fe7-a53f-e8b2bf2af4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 116)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "028c797d-81a6-428f-8887-c92e9717753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = b'\\x01\\x02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "81c7b9e3-862d-442d-88ee-b15962381604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f7b372b5-45dd-49bb-81ea-21698facc3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b' t'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(max_bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddccc2c0-8fcb-406c-b313-29cf77c20bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d69c9279-fb9d-4014-bee3-fcc03f9c13ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "with open('./output/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca8855-96ab-4b3d-9fd3-50356eaf34e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output/merges.pkl', 'rb') as f:\n",
    "    merges = pickle.load(f)\n",
    "\n",
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897a19c-64b7-4a6f-839f-e0b58045af15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
